A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)
[Powered by Stella]
Using device: cpu
Selected model: base (DQN)

============================================================
Network Architecture:
============================================================
DQN(
  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
  (fc1): Linear(in_features=3136, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=4, bias=True)
)
============================================================

Starting training with 2000 episodes...
Warmup steps: 1000
Target network updates every 1000 steps

âœ“ Training started!


ğŸ“Š Last 50 episodes avg: Reward=0.9, Loss=0.0022

Episode 100/2000 | Reward:    3.0 | Steps:  237 | Loss: 0.0029 | Epsilon: 0.843 | Buffer:  17474 | Time: 17.9s

ğŸ“Š Last 50 episodes avg: Reward=1.2, Loss=0.0027


ğŸ“Š Last 50 episodes avg: Reward=1.7, Loss=0.0033

Episode 200/2000 | Reward:    0.0 | Steps:  138 | Loss: 0.0045 | Epsilon: 0.662 | Buffer:  37543 | Time: 7.8s

ğŸ“Š Last 50 episodes avg: Reward=2.0, Loss=0.0038


ğŸ“Š Last 50 episodes avg: Reward=1.8, Loss=0.0041

Episode 300/2000 | Reward:    2.0 | Steps:  223 | Loss: 0.0052 | Epsilon: 0.476 | Buffer:  50000 | Time: 10.5s

ğŸ“Š Last 50 episodes avg: Reward=1.9, Loss=0.0044


ğŸ“Š Last 50 episodes avg: Reward=2.0, Loss=0.0052

Episode 400/2000 | Reward:    3.0 | Steps:  223 | Loss: 0.0052 | Epsilon: 0.280 | Buffer:  50000 | Time: 11.0s

ğŸ“Š Last 50 episodes avg: Reward=1.9, Loss=0.0054


ğŸ“Š Last 50 episodes avg: Reward=2.1, Loss=0.0061

Episode 500/2000 | Reward:    3.0 | Steps:  259 | Loss: 0.0068 | Epsilon: 0.100 | Buffer:  50000 | Time: 12.6s

ğŸ“Š Last 50 episodes avg: Reward=2.0, Loss=0.0061


ğŸ“Š Last 50 episodes avg: Reward=2.2, Loss=0.0062

Episode 600/2000 | Reward:    0.0 | Steps:  130 | Loss: 0.0058 | Epsilon: 0.100 | Buffer:  50000 | Time: 7.4s

ğŸ“Š Last 50 episodes avg: Reward=1.8, Loss=0.0064


ğŸ“Š Last 50 episodes avg: Reward=1.8, Loss=0.0063

Episode 700/2000 | Reward:    0.0 | Steps:  149 | Loss: 0.0055 | Epsilon: 0.100 | Buffer:  50000 | Time: 8.4s

ğŸ“Š Last 50 episodes avg: Reward=1.8, Loss=0.0061


ğŸ“Š Last 50 episodes avg: Reward=2.0, Loss=0.0064

Episode 800/2000 | Reward:    0.0 | Steps:  150 | Loss: 0.0072 | Epsilon: 0.100 | Buffer:  50000 | Time: 8.3s

ğŸ“Š Last 50 episodes avg: Reward=1.5, Loss=0.0060


ğŸ“Š Last 50 episodes avg: Reward=1.6, Loss=0.0057

Episode 900/2000 | Reward:    3.0 | Steps:  237 | Loss: 0.0052 | Epsilon: 0.100 | Buffer:  50000 | Time: 14.4s

ğŸ“Š Last 50 episodes avg: Reward=2.1, Loss=0.0055


ğŸ“Š Last 50 episodes avg: Reward=2.1, Loss=0.0056

Episode 1000/2000 | Reward:    4.0 | Steps:  284 | Loss: 0.0056 | Epsilon: 0.100 | Buffer:  50000 | Time: 16.0s

ğŸ“Š Last 50 episodes avg: Reward=1.9, Loss=0.0057


ğŸ“Š Last 50 episodes avg: Reward=1.8, Loss=0.0055

Episode 1100/2000 | Reward:    4.0 | Steps:  309 | Loss: 0.0051 | Epsilon: 0.100 | Buffer:  50000 | Time: 19.1s

ğŸ“Š Last 50 episodes avg: Reward=2.4, Loss=0.0053


ğŸ“Š Last 50 episodes avg: Reward=2.2, Loss=0.0050

Episode 1200/2000 | Reward:    2.0 | Steps:  206 | Loss: 0.0055 | Epsilon: 0.100 | Buffer:  50000 | Time: 9.1s

ğŸ“Š Last 50 episodes avg: Reward=1.9, Loss=0.0048


ğŸ“Š Last 50 episodes avg: Reward=2.4, Loss=0.0048

Episode 1300/2000 | Reward:    3.0 | Steps:  267 | Loss: 0.0038 | Epsilon: 0.100 | Buffer:  50000 | Time: 11.8s

ğŸ“Š Last 50 episodes avg: Reward=2.6, Loss=0.0043


ğŸ“Š Last 50 episodes avg: Reward=2.2, Loss=0.0037

Episode 1400/2000 | Reward:    2.0 | Steps:  275 | Loss: 0.0028 | Epsilon: 0.100 | Buffer:  50000 | Time: 12.2s

ğŸ“Š Last 50 episodes avg: Reward=2.4, Loss=0.0031


ğŸ“Š Last 50 episodes avg: Reward=2.5, Loss=0.0026

Episode 1500/2000 | Reward:    2.0 | Steps:  199 | Loss: 0.0025 | Epsilon: 0.100 | Buffer:  50000 | Time: 9.3s

ğŸ“Š Last 50 episodes avg: Reward=2.9, Loss=0.0023


ğŸ“Š Last 50 episodes avg: Reward=2.1, Loss=0.0022

Episode 1600/2000 | Reward:    7.0 | Steps:  321 | Loss: 0.0016 | Epsilon: 0.100 | Buffer:  50000 | Time: 31.9s

ğŸ“Š Last 50 episodes avg: Reward=5.9, Loss=0.0019


ğŸ“Š Last 50 episodes avg: Reward=5.8, Loss=0.0015

Episode 1700/2000 | Reward:    1.0 | Steps:  536 | Loss: 0.0015 | Epsilon: 0.100 | Buffer:  50000 | Time: 23.8s

ğŸ“Š Last 50 episodes avg: Reward=2.7, Loss=0.0015


ğŸ“Š Last 50 episodes avg: Reward=2.1, Loss=0.0014

Episode 1800/2000 | Reward:    3.0 | Steps:  231 | Loss: 0.0017 | Epsilon: 0.100 | Buffer:  50000 | Time: 932.9s

ğŸ“Š Last 50 episodes avg: Reward=5.0, Loss=0.0017


ğŸ“Š Last 50 episodes avg: Reward=4.2, Loss=0.0016

Episode 1900/2000 | Reward:    3.0 | Steps:  226 | Loss: 0.0012 | Epsilon: 0.100 | Buffer:  50000 | Time: 17.2s

ğŸ“Š Last 50 episodes avg: Reward=3.7, Loss=0.0017


ğŸ“Š Last 50 episodes avg: Reward=3.7, Loss=0.0015

Episode 2000/2000 | Reward:    1.0 | Steps:  155 | Loss: 0.0014 | Epsilon: 0.100 | Buffer:  50000 | Time: 9.2s

ğŸ“Š Last 50 episodes avg: Reward=3.5, Loss=0.0014


ğŸ’¾ Model saved to: checkpoints/base_final.pt
ğŸ“Š Training logs saved to: checkpoints/base_logs.json

============================================================
Training Complete!
============================================================
Model: base
Total episodes: 2000
Total steps: 492288
Training time: 33h 36m 34s
Average reward: 2.46
Best episode reward: 11.00
Final epsilon: 0.100

ğŸ“ Saved files:
   - checkpoints/base_final.pt
   - checkpoints/base_logs.json

ğŸ“ˆ View training metrics: tensorboard --logdir=runs
============================================================
